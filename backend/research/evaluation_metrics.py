"""
Evaluation Metrics for NL2SQL Research
Implements multiple metrics to evaluate SQL generation quality
"""

import re
import difflib
import psycopg2
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
import time
from collections import Counter
import os
from dotenv import load_dotenv

load_dotenv()


@dataclass
class EvaluationResult:
    """Container for evaluation results"""
    question_id: str
    question: str
    category: str
    complexity: str

    # Execution metrics
    execution_success: bool
    execution_time: float
    execution_error: Optional[str]

    # SQL quality metrics
    sql_similarity_score: float
    exact_match: bool
    bleu_score: float

    # Semantic correctness
    results_match: bool
    row_count_match: bool
    column_count_match: bool
    data_similarity_score: float

    # Pattern analysis
    pattern_correctness: Dict[str, bool]
    missing_components: List[str]
    extra_components: List[str]

    # Generated vs Ground truth
    generated_sql: str
    ground_truth_sql: str

    # Additional metadata
    num_few_shot_examples: int
    model_name: str
    timestamp: str


class EvaluationMetrics:
    """
    Comprehensive evaluation metrics for NL2SQL systems
    """

    def __init__(self):
        """Initialize database connection"""
        self.conn_params = {
            'host': os.getenv('PGHOST', 'localhost'),
            'port': os.getenv('PGPORT', 5432),
            'database': os.getenv('PGDATABASE', 'financial_db'),
            'user': os.getenv('PGUSER'),
            'password': os.getenv('PGPASSWORD')
        }

    def evaluate_query(
        self,
        question_id: str,
        question: str,
        category: str,
        complexity: str,
        generated_sql: str,
        ground_truth_sql: str,
        num_few_shot: int = 15,
        model_name: str = "llama-3.3-70b"
    ) -> EvaluationResult:
        """
        Comprehensive evaluation of a generated SQL query

        Args:
            question_id: Unique identifier for the question
            question: Natural language question
            category: Question category
            complexity: Complexity level (simple/medium/complex)
            generated_sql: SQL generated by the system
            ground_truth_sql: Expected correct SQL
            num_few_shot: Number of few-shot examples used
            model_name: LLM model name

        Returns:
            EvaluationResult with comprehensive metrics
        """

        # Ensure generated_sql is not None
        if generated_sql is None:
            generated_sql = ""

        # Execute both queries
        gen_success, gen_result, gen_time, gen_error = self._execute_query(generated_sql)
        gt_success, gt_result, gt_time, gt_error = self._execute_query(ground_truth_sql)

        # SQL similarity metrics
        sql_similarity = self._calculate_sql_similarity(generated_sql, ground_truth_sql)
        exact_match = self._check_exact_match(generated_sql, ground_truth_sql)
        bleu = self._calculate_bleu_score(generated_sql, ground_truth_sql)

        # Semantic correctness (compare results)
        results_match = False
        row_match = False
        col_match = False
        data_similarity = 0.0

        if gen_success and gt_success:
            results_match, row_match, col_match, data_similarity = self._compare_results(
                gen_result, gt_result
            )

        # Pattern analysis
        pattern_correct, missing, extra = self._analyze_sql_patterns(
            generated_sql, ground_truth_sql
        )

        return EvaluationResult(
            question_id=question_id,
            question=question,
            category=category,
            complexity=complexity,
            execution_success=gen_success,
            execution_time=gen_time,
            execution_error=gen_error,
            sql_similarity_score=sql_similarity,
            exact_match=exact_match,
            bleu_score=bleu,
            results_match=results_match,
            row_count_match=row_match,
            column_count_match=col_match,
            data_similarity_score=data_similarity,
            pattern_correctness=pattern_correct,
            missing_components=missing,
            extra_components=extra,
            generated_sql=generated_sql if generated_sql else "",
            ground_truth_sql=ground_truth_sql,
            num_few_shot_examples=num_few_shot,
            model_name=model_name,
            timestamp=pd.Timestamp.now().isoformat()
        )

    def _execute_query(self, sql: str) -> Tuple[bool, Optional[pd.DataFrame], float, Optional[str]]:
        """
        Execute SQL query and return results

        Returns:
            (success, result_df, execution_time, error_message)
        """
        # Handle None or empty SQL
        if sql is None or not sql or not sql.strip():
            return False, None, 0.0, "Empty or None SQL query"

        try:
            conn = psycopg2.connect(**self.conn_params)

            start_time = time.time()
            df = pd.read_sql_query(sql, conn)
            execution_time = time.time() - start_time

            conn.close()
            return True, df, execution_time, None

        except Exception as e:
            return False, None, 0.0, str(e)

    def _calculate_sql_similarity(self, sql1: str, sql2: str) -> float:
        """
        Calculate similarity between two SQL queries using sequence matching
        Returns score between 0 and 1
        """
        # Normalize SQL strings
        normalized1 = self._normalize_sql(sql1)
        normalized2 = self._normalize_sql(sql2)

        # Use difflib's SequenceMatcher
        similarity = difflib.SequenceMatcher(None, normalized1, normalized2).ratio()
        return round(similarity, 4)

    def _normalize_sql(self, sql: str) -> str:
        """Normalize SQL for comparison"""
        # Handle None or empty SQL
        if sql is None or not sql:
            return ""

        # Convert to lowercase
        sql = sql.lower()
        # Remove extra whitespace
        sql = re.sub(r'\s+', ' ', sql)
        # Remove comments
        sql = re.sub(r'--.*?\n', '', sql)
        sql = re.sub(r'/\*.*?\*/', '', sql, flags=re.DOTALL)
        # Trim
        sql = sql.strip()
        return sql

    def _check_exact_match(self, sql1: str, sql2: str) -> bool:
        """Check if two SQL queries are exactly the same (after normalization)"""
        return self._normalize_sql(sql1) == self._normalize_sql(sql2)

    def _calculate_bleu_score(self, generated: str, reference: str) -> float:
        """
        Calculate BLEU-like score for SQL comparison
        Uses token-level comparison
        """
        # Tokenize SQL
        gen_tokens = self._tokenize_sql(generated)
        ref_tokens = self._tokenize_sql(reference)

        if not gen_tokens or not ref_tokens:
            return 0.0

        # Calculate precision
        gen_counter = Counter(gen_tokens)
        ref_counter = Counter(ref_tokens)

        overlap = sum((gen_counter & ref_counter).values())
        precision = overlap / len(gen_tokens) if gen_tokens else 0.0

        # Calculate recall
        recall = overlap / len(ref_tokens) if ref_tokens else 0.0

        # F1-like score
        if precision + recall == 0:
            return 0.0

        f1 = 2 * (precision * recall) / (precision + recall)
        return round(f1, 4)

    def _tokenize_sql(self, sql: str) -> List[str]:
        """Tokenize SQL into keywords and identifiers"""
        normalized = self._normalize_sql(sql)
        # Split on whitespace and special characters
        tokens = re.findall(r'\w+|[^\w\s]', normalized)
        return tokens

    def _compare_results(
        self,
        df1: pd.DataFrame,
        df2: pd.DataFrame
    ) -> Tuple[bool, bool, bool, float]:
        """
        Compare two result DataFrames

        Returns:
            (exact_match, row_count_match, column_count_match, similarity_score)
        """
        # Check dimensions
        row_match = len(df1) == len(df2)
        col_match = len(df1.columns) == len(df2.columns)

        # Try to compare data
        try:
            # Sort both dataframes for comparison
            df1_sorted = df1.sort_values(by=list(df1.columns)).reset_index(drop=True)
            df2_sorted = df2.sort_values(by=list(df2.columns)).reset_index(drop=True)

            # Check if completely equal
            exact_match = df1_sorted.equals(df2_sorted)

            # Calculate similarity score
            if exact_match:
                similarity = 1.0
            else:
                # Compare cell by cell for numeric similarity
                matching_cells = 0
                total_cells = df1_sorted.size

                if df1_sorted.shape == df2_sorted.shape:
                    for col in df1_sorted.columns:
                        if col in df2_sorted.columns:
                            # For numeric columns, allow small tolerance
                            if pd.api.types.is_numeric_dtype(df1_sorted[col]):
                                matching_cells += (
                                    (df1_sorted[col].round(2) == df2_sorted[col].round(2)).sum()
                                )
                            else:
                                matching_cells += (df1_sorted[col] == df2_sorted[col]).sum()

                    similarity = matching_cells / total_cells if total_cells > 0 else 0.0
                else:
                    similarity = 0.0

            return exact_match, row_match, col_match, round(similarity, 4)

        except Exception as e:
            return False, row_match, col_match, 0.0

    def _analyze_sql_patterns(
        self,
        generated: str,
        ground_truth: str
    ) -> Tuple[Dict[str, bool], List[str], List[str]]:
        """
        Analyze which SQL patterns are correctly used

        Returns:
            (pattern_correctness_dict, missing_components, extra_components)
        """
        patterns = {
            'has_select': r'\bSELECT\b',
            'has_from': r'\bFROM\b',
            'has_join': r'\bJOIN\b',
            'has_where': r'\bWHERE\b',
            'has_group_by': r'\bGROUP BY\b',
            'has_order_by': r'\bORDER BY\b',
            'has_cte': r'\bWITH\b',
            'has_window_func': r'\b(LAG|LEAD|ROW_NUMBER|RANK|DENSE_RANK|FIRST_VALUE|LAST_VALUE)\b',
            'has_case': r'\bCASE\b',
            'has_subquery': r'SELECT.*FROM.*SELECT',
            'has_aggregate': r'\b(SUM|AVG|COUNT|MIN|MAX|STDDEV)\b',
        }

        gen_normalized = self._normalize_sql(generated).upper()
        gt_normalized = self._normalize_sql(ground_truth).upper()

        pattern_correctness = {}
        missing = []
        extra = []

        for pattern_name, pattern_regex in patterns.items():
            gen_has = bool(re.search(pattern_regex, gen_normalized, re.IGNORECASE))
            gt_has = bool(re.search(pattern_regex, gt_normalized, re.IGNORECASE))

            # Pattern is correct if both have it or both don't have it
            pattern_correctness[pattern_name] = gen_has == gt_has

            if gt_has and not gen_has:
                missing.append(pattern_name)
            elif gen_has and not gt_has:
                extra.append(pattern_name)

        return pattern_correctness, missing, extra

    def batch_evaluate(
        self,
        test_cases: List[Dict[str, Any]],
        num_few_shot: int = 15,
        model_name: str = "llama-3.3-70b"
    ) -> pd.DataFrame:
        """
        Evaluate multiple test cases and return results as DataFrame

        Args:
            test_cases: List of dicts with keys: question_id, question, category,
                       complexity, generated_sql, ground_truth_sql
            num_few_shot: Number of few-shot examples used
            model_name: Model name

        Returns:
            DataFrame with all evaluation results
        """
        results = []

        for test_case in test_cases:
            result = self.evaluate_query(
                question_id=test_case['question_id'],
                question=test_case['question'],
                category=test_case['category'],
                complexity=test_case['complexity'],
                generated_sql=test_case['generated_sql'],
                ground_truth_sql=test_case['ground_truth_sql'],
                num_few_shot=num_few_shot,
                model_name=model_name
            )
            results.append(result)

        # Convert to DataFrame
        df = pd.DataFrame([vars(r) for r in results])
        return df

    def calculate_aggregate_metrics(self, results_df: pd.DataFrame) -> Dict[str, float]:
        """
        Calculate aggregate metrics from evaluation results

        Returns:
            Dictionary with overall performance metrics
        """
        total = len(results_df)

        metrics = {
            # Execution metrics
            'execution_accuracy': (results_df['execution_success'].sum() / total) * 100,
            'avg_execution_time': results_df[results_df['execution_success']]['execution_time'].mean(),

            # Correctness metrics
            'exact_match_rate': (results_df['exact_match'].sum() / total) * 100,
            'results_match_rate': (results_df['results_match'].sum() / total) * 100,
            'semantic_correctness': (results_df['data_similarity_score'].mean()) * 100,

            # SQL quality metrics
            'avg_sql_similarity': results_df['sql_similarity_score'].mean() * 100,
            'avg_bleu_score': results_df['bleu_score'].mean() * 100,

            # By complexity
            'simple_accuracy': self._accuracy_by_complexity(results_df, 'simple'),
            'medium_accuracy': self._accuracy_by_complexity(results_df, 'medium'),
            'complex_accuracy': self._accuracy_by_complexity(results_df, 'complex'),

            # By category
            'category_breakdown': self._accuracy_by_category(results_df),
        }

        return metrics

    def _accuracy_by_complexity(self, df: pd.DataFrame, complexity: str) -> float:
        """Calculate execution accuracy for a specific complexity level"""
        subset = df[df['complexity'] == complexity]
        if len(subset) == 0:
            return 0.0
        return (subset['execution_success'].sum() / len(subset)) * 100

    def _accuracy_by_category(self, df: pd.DataFrame) -> Dict[str, float]:
        """Calculate execution accuracy by category"""
        categories = df['category'].unique()
        breakdown = {}

        for cat in categories:
            subset = df[df['category'] == cat]
            accuracy = (subset['execution_success'].sum() / len(subset)) * 100
            breakdown[cat] = round(accuracy, 2)

        return breakdown


if __name__ == "__main__":
    # Test the evaluation metrics
    evaluator = EvaluationMetrics()

    # Example test
    test_sql = """
    SELECT c.name, fp.fiscal_year, ff.value
    FROM financial_fact ff
    JOIN statement s ON ff.statement_id = s.statement_id
    JOIN fiscal_period fp ON s.period_id = fp.period_id
    JOIN company c ON fp.company_id = c.company_id
    JOIN line_item li ON ff.line_item_id = li.line_item_id
    WHERE li.normalized_code = 'HUL_PROFIT_LOSS_REVENUE_FROM_OPERATIONS_NET'
    AND fp.fiscal_year = 2024;
    """

    result = evaluator.evaluate_query(
        question_id="TEST001",
        question="What was revenue in 2024?",
        category="revenue_analysis",
        complexity="simple",
        generated_sql=test_sql,
        ground_truth_sql=test_sql,
        num_few_shot=15
    )

    print("Evaluation Result:")
    print(f"Execution Success: {result.execution_success}")
    print(f"SQL Similarity: {result.sql_similarity_score}")
    print(f"Results Match: {result.results_match}")
